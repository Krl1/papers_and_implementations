{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/segment_anything.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LayerNorm2d = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptEncoder:\n",
    "    def __init__(self):\n",
    "        self.pe_layer = None\n",
    "        self.input_image_size = None\n",
    "        self.not_a_point_embed = None\n",
    "        self.point_embeddings = None\n",
    "        self.image_embedding_size = None\n",
    "    \n",
    "    def _embed_points(\n",
    "            self,\n",
    "            points: torch.Tensor, # Indicates the coordinates of the points\n",
    "            labels: torch.Tensor, # Indicate if th epoint is foreground or background\n",
    "            pad: bool\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Embeds point prompts.\"\"\"\n",
    "        points = points + 0.5 # Shift to center of pixel\n",
    "        if pad: # Add padding if needed (to keep the segment length constant)\n",
    "            padding_point = torch.zeros((points.shape[0], 1, 2), device=points.device)\n",
    "            padding_label = -torch.ones((labels.shape[0], 1), device=labels.device) # The -1 label indicates padding\n",
    "            points = torch.cat([points, padding_point], dim=1) # Append the padding point\n",
    "            labels = torch.cat([labels, padding_label], dim=1) # Append the padding label\n",
    "        \n",
    "        point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size) # Obtain the positional encodings\n",
    "        point_embedding[labels == -1] = 0.0 # Zero out the padding points\n",
    "        point_embedding[labels == -1] += self.not_a_point_embed.weight # Add special embedding to indicate padding\n",
    "        point_embedding[labels == 0] += self.point_embeddings[0].weight # Add embedding for background points\n",
    "        point_embedding[labels == 1] += self.point_embeddings[1].weight # Add embedding for foreground points\n",
    "        return point_embedding\n",
    "    \n",
    "    def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Embeds box prompts.\"\"\"\n",
    "        boxes = boxes + 0.5 # Shift to center of pixel\n",
    "        coords = boxes.reshape(-1, 2, 2)\n",
    "        corner_embedding = self.pe_layer.forward_with_coords(coords, self.input_image_size) # Obtain the positional encodings\n",
    "        corner_embedding[:, 0, :] += self.point_embeddings[2].weight # Special embedding for top-left corner\n",
    "        corner_embedding[:, 1, :] += self.point_embeddings[3].weight # Special embedding for bottom-right corner\n",
    "        return corner_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mask:\n",
    "    def __init__(self, mask_in_chans, embed_dim, activation):\n",
    "        self._init_mask(mask_in_chans, embed_dim, activation)\n",
    "    \n",
    "    def __expand_image_to_mask(self, image_embeddings, tokens, dense_prompt_embeddings, image_pe):\n",
    "        # Expand per-image data in batch direction to be per-mask\n",
    "        src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)\n",
    "        src = src + dense_prompt_embeddings # Add mask embeddings to the image\n",
    "        pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)\n",
    "        b, c, h, w = src.shape \n",
    "\n",
    "    def __get_dense_embeddings(self, masks: Optional[torch.Tensor], bs):\n",
    "        if masks is not None:\n",
    "            dense_embeddings = self._embed_masks(masks)\n",
    "        else: # If no mask is specified, use a special \"no mask\" embedding\n",
    "            dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1).expand(\n",
    "                bs, -1, self.image_embedding_size[0], self.image_embedding_size[1]\n",
    "            )\n",
    "        return dense_embeddings\n",
    "\n",
    "    def _init_mask(self, mask_in_chans, embed_dim, activation):\n",
    "        self.mask_downscaling = nn.Sequential(\n",
    "            nn.Conv2d(1, mask_in_chans // 4, kernel_size=2, stride=2),\n",
    "            LayerNorm2d(mask_in_chans // 4),\n",
    "            activation(),\n",
    "            nn.Conv2d(mask_in_chans // 4, mask_in_chans, kernel_size=2, stride=2),\n",
    "            LayerNorm2d(mask_in_chans),\n",
    "            activation(),\n",
    "            nn.Conv2d(mask_in_chans, embed_dim, kernel_size=1),\n",
    "        )\n",
    "        self.no_mask_embed == nn.Embedding(1, embed_dim)\n",
    "\n",
    "    def _embed_masks(self, masks: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Embeds mask prompts.\"\"\"\n",
    "        masks_embedding = self.mask_downscaling(masks)\n",
    "        return masks_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/segment_anything_mask_decoder.png\" alt=\"drawing\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDecoder:\n",
    "    def __init__(self):\n",
    "        self.layers = None\n",
    "\n",
    "    def tmp(self, point_embedding, image_embedding, image_pe):\n",
    "        # Prepare queries\n",
    "        queries = point_embedding\n",
    "        keys = image_embedding\n",
    "\n",
    "        # Apply transformer blocks and final layernorm\n",
    "        for layer in self.layers:\n",
    "            queries, keys = layer(\n",
    "                queries=queries, \n",
    "                keys=keys,\n",
    "                query_pe=point_embedding,\n",
    "                key_pe=image_pe,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
