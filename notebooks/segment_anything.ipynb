{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/segment_anything.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, List\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LayerNorm2d = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptEncoder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _embed_points(\n",
    "            self,\n",
    "            points: torch.Tensor, # Indicates the coordinates of the points\n",
    "            labels: torch.Tensor, # Indicate if th epoint is foreground or background\n",
    "            pad: bool\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Embeds point prompts.\"\"\"\n",
    "        points = points + 0.5 # Shift to center of pixel\n",
    "        if pad: # Add padding if needed (to keep the segment length constant)\n",
    "            padding_point = torch.zeros((points.shape[0], 1, 2), device=points.device)\n",
    "            padding_label = -torch.ones((labels.shape[0], 1), device=labels.device) # The -1 label indicates padding\n",
    "            points = torch.cat([points, padding_point], dim=1) # Append the padding point\n",
    "            labels = torch.cat([labels, padding_label], dim=1) # Append the padding label\n",
    "        \n",
    "        point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size) # Obtain the positional encodings\n",
    "        point_embedding[labels == -1] = 0.0 # Zero out the padding points\n",
    "        point_embedding[labels == -1] += self.not_a_point_embed.weight # Add special embedding to indicate padding\n",
    "        point_embedding[labels == 0] += self.point_embeddings[0].weight # Add embedding for background points\n",
    "        point_embedding[labels == 1] += self.point_embeddings[1].weight # Add embedding for foreground points\n",
    "        return point_embedding\n",
    "    \n",
    "    def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Embeds box prompts.\"\"\"\n",
    "        boxes = boxes + 0.5 # Shift to center of pixel\n",
    "        coords = boxes.reshape(-1, 2, 2)\n",
    "        corner_embedding = self.pe_layer.forward_with_coords(coords, self.input_image_size) # Obtain the positional encodings\n",
    "        corner_embedding[:, 0, :] += self.point_embeddings[2].weight # Special embedding for top-left corner\n",
    "        corner_embedding[:, 1, :] += self.point_embeddings[3].weight # Special embedding for bottom-right corner\n",
    "        return corner_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mask:\n",
    "    def __init__(self, mask_in_chans, embed_dim, activation):\n",
    "        self._init_mask(mask_in_chans, embed_dim, activation)\n",
    "    \n",
    "    def __expand_image_to_mask(self, image_embeddings, tokens, dense_prompt_embeddings, image_pe):\n",
    "        # Expand per-image data in batch direction to be per-mask\n",
    "        src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)\n",
    "        src = src + dense_prompt_embeddings # Add mask embeddings to the image\n",
    "        pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)\n",
    "        b, c, h, w = src.shape \n",
    "\n",
    "    def __get_dense_embeddings(self, masks: Optional[torch.Tensor], bs):\n",
    "        if masks is not None:\n",
    "            dense_embeddings = self._embed_masks(masks)\n",
    "        else: # If no mask is specified, use a special \"no mask\" embedding\n",
    "            dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1).expand(\n",
    "                bs, -1, self.image_embedding_size[0], self.image_embedding_size[1]\n",
    "            )\n",
    "        return dense_embeddings\n",
    "\n",
    "    def _init_mask(self, mask_in_chans, embed_dim, activation):\n",
    "        self.mask_downscaling = nn.Sequential(\n",
    "            nn.Conv2d(1, mask_in_chans // 4, kernel_size=2, stride=2),\n",
    "            LayerNorm2d(mask_in_chans // 4),\n",
    "            activation(),\n",
    "            nn.Conv2d(mask_in_chans // 4, mask_in_chans, kernel_size=2, stride=2),\n",
    "            LayerNorm2d(mask_in_chans),\n",
    "            activation(),\n",
    "            nn.Conv2d(mask_in_chans, embed_dim, kernel_size=1),\n",
    "        )\n",
    "        self.no_mask_embed == nn.Embedding(1, embed_dim)\n",
    "\n",
    "    def _embed_masks(self, masks: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Embeds mask prompts.\"\"\"\n",
    "        masks_embedding = self.mask_downscaling(masks)\n",
    "        return masks_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/segment_anything_mask_decoder.png\" alt=\"drawing\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_token = torch.cat([self.iou_token.weight, self.mask_tokens.weight], dim=0)\n",
    "output_token = output_token.unsqueeze(0).expand(sparse_prompt_embedding.size(0), -1, -1)\n",
    "tokens = torch.cat([output_token, sparse_prompt_embeddings], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDecoder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _tmp(self, point_embedding, image_embedding, image_pe):\n",
    "        # Prepare queries\n",
    "        queries = point_embedding\n",
    "        keys = image_embedding\n",
    "\n",
    "        # Apply transformer blocks and final layernorm\n",
    "        for layer in self.layers:\n",
    "            queries, keys = layer(\n",
    "                queries=queries, \n",
    "                keys=keys,\n",
    "                query_pe=point_embedding,\n",
    "                key_pe=image_pe,\n",
    "            )\n",
    "\n",
    "    def _prepare_tokens(self, sparse_prompt_embeddings):\n",
    "        output_token = torch.cat([self.iou_token.weight, self.mask_tokens.weight], dim=0)\n",
    "        output_token = output_token.unsqueeze(0).expand(sparse_prompt_embeddings.size(0), -1, -1)\n",
    "        tokens = torch.cat([output_token, sparse_prompt_embeddings], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            queries: torch.Tensor,\n",
    "            keys: torch.Tensor,\n",
    "            query_pe: torch.Tensor,\n",
    "            key_pe: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Self attention block\n",
    "        if self.skip_first_layer_pe:\n",
    "            queries = self.self_attn(q=queries, k=queries, v=queries)\n",
    "        else:\n",
    "            q = queries + query_pe # Add positional encoding to the prompt tokens\n",
    "            attn_out = self.self_attn(q=q, k=q, v=queries) # Run self-attention on the prompt\n",
    "            queries = queries + attn_out\n",
    "        queries = self.norm1(queries)\n",
    "\n",
    "        # Cross attention block, tokens attending to image embedding\n",
    "        q = queries + query_pe # The queries are the prompt tokens\n",
    "        k = keys + key_pe # The keys are the image embedding + positional encoding\n",
    "        # the vaules are the image embedding (without positional encoding)\n",
    "        attn_out = self.cross_attn_token_to_image(q=q, k=k, v=keys)\n",
    "        queries = queries + attn_out\n",
    "        queries = self.norm2(queries)\n",
    "\n",
    "        # MLP block\n",
    "        mlp_out = self.mlp(queries)\n",
    "        queries = queries + mlp_out\n",
    "        queries = self.norm3(queries)\n",
    "\n",
    "        # Cross attention block, image embedding attending to tokens\n",
    "        q = queries + query_pe # q = prompt tokens + positional encoding\n",
    "        k = keys + key_pe # k = image embedding + positional encoding\n",
    "        # Bad variable naming practices from META!\n",
    "        attn_out = self.cross_attn_image_to_token(q=k, k=q, v=queries)\n",
    "        keys = keys + attn_out\n",
    "        keys = self.norm4(keys)\n",
    "\n",
    "        return queries, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, src, pos_src, tokens, hs, b, c, h, w):\n",
    "        # Run the transformer\n",
    "        # hs is the transformer output for the prompt\n",
    "        # src is the tranformer output for the image\n",
    "\n",
    "        hs, src = self.tranformer(src, pos_src, tokens)\n",
    "        iou_token_out = hs[:, 0, :] # Output token for the IoU prediction\n",
    "        mask_tokens_out = hs [:, 1 : (1 + self.num_mask_tokens), :] # Output tokens for the mask\n",
    "\n",
    "        # Upscale mask embedding and predict masks using the mask tokens\n",
    "        src = src.transpose(1,2).view(b, c, h, w)\n",
    "        upscale_embedding = self.output_upscaling(src)\n",
    "        hyper_in_list: List[torch.Tensor] = []\n",
    "        for i in range(self.num_mask_tokens): # Run each token through its MLP \n",
    "            hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :]))\n",
    "        hyper_in = torch.stack(hyper_in_list, dim=1)\n",
    "        b, c, h, w = upscale_embedding.shape\n",
    "        # Dot product of the MLP output for each \"output token\" and the upscale image\n",
    "        # (each output token represents a mask)\n",
    "        masks = (hyper_in @ upscale_embedding.view(b, c, h * w)).view(b, -1, h, w)\n",
    "\n",
    "        # Generate mask quality predictions\n",
    "        iou_pred = self.iou_prediction_head(iou_token_out)\n",
    "\n",
    "        return masks, iou_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
